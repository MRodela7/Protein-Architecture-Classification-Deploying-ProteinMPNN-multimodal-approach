# -*- coding: utf-8 -*-
"""Copy of getting_started.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hwr9KkHvFRLbuokqlOh6aoNi-Q8EPFZd
"""

def save_structure_as_image(pdb_filename, image_filename, view_options=None):
    view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)
    view.addModel(open(pdb_filename, 'r').read(), 'pdb')
    if view_options:
        view.setStyle(view_options)
    view.zoomTo()
    view.show()
    view.png()  # You might need to adjust this part to ensure the image gets saved properly.
    # Saving the image - This part needs to be adapted based on how you retrieve the image data from py3Dmol.
    plt.imshow(view.getImage())  # Assuming `getImage` retrieves the image; adjust as necessary.
    plt.axis('off')
    plt.savefig(image_filename)

"""# ML Hands-on Challenge - Getting started

The goal of this notebook is to explore the data that we have provided from the ML hands-on challenge. You will learn more about the CATH labels, how to visualize the protein structure, and challenges you will have to handle (e.g. gaps in structure).

## 0. Setup
"""

!pip install biopython

!pip install py3dmol

import pandas as pd
import numpy as np
import sys
import glob
import os
import Bio.PDB.PDBParser
import py3Dmol
import warnings

warnings.filterwarnings("ignore", message="Used element '.' for Atom")

architecture_names = {(1, 10): "Mainly Alpha: Orthogonal Bundle",
                      (1, 20): "Mainly Alpha: Up-down Bundle",
                      (2, 30): "Mainly Beta: Roll",
                      (2, 40): "Mainly Beta: Beta Barrel",
                      (2, 60): "Mainly Beta: Sandwich",
                      (3, 10): "Alpha Beta: Roll",
                      (3, 20): "Alpha Beta: Alpha-Beta Barrel",
                      (3, 30): "Alpha Beta: 2-Layer Sandwich",
                      (3, 40): "Alpha Beta: 3-Layer(aba) Sandwich",
                      (3, 90): "Alpha Beta: Alpha-Beta Complex"}

"""## 1. Opening the data"""

# Open the training data sequences and structure
data = pd.read_csv('/content/cath_w_seqs_share.csv', index_col=0)
data

# The CATH classification for a protein can be determined by concatenating the columns
example_cath_id = data['cath_id'][0]
print(example_cath_id)
example_class = data['class'][0]
print(example_class)
example_arch = f"({example_class},{data['architecture'][0]})"
print(example_arch)
example_topo = f"({example_class},{data['architecture'][0]},{data['topology'][0]})"
print(example_topo)
example_superfam = f"({example_class},{data['architecture'][0]},{data['topology'][0]},{data['superfamily'][0]})"
print(example_superfam)
print(f"""
Protein domain with cath id {example_cath_id} is in class {example_class}, \
architecture {example_arch}, topology {example_topo}, and superfamily {example_superfam}.
""")

# The sequences come from the PDB files
from Bio.PDB.Polypeptide import protein_letters_3to1

def get_sequence_from_pdb(pdb_filename):
    pdb_parser = Bio.PDB.PDBParser()
    structure = pdb_parser.get_structure(pdb_filename, pdb_filename)
    assert len(structure) == 1

    seq = []

    for model in structure:
        for chain in model:
            for residue in chain:
                if residue.get_id()[0] == " ":  # This checks if it's a standard residue
                    seq.append(protein_letters_3to1[residue.get_resname()])
                else:
                    print('nonstandard', residue.get_id())

    return ''.join(seq)

pdb_path = '/content/drive/MyDrive/PDB_files/pdb_share'
example_seq = get_sequence_from_pdb(f"{pdb_path}/{example_cath_id}")
print(f"The sequence for cath id {example_cath_id} is {example_seq}")

# Check that it matches the data file
data['sequences'][0] == example_seq

"""## 2. Visualize some examples"""

# Load sequence and structure for one example for each architecture
cath_examples = data.groupby(['class', 'architecture'])[['cath_id', 'sequences']].first()
cath_examples

list(cath_examples)

def view_structure(pdb_filename, name, gaps=[], width=300, height=300):
  pdb_parser = Bio.PDB.PDBParser()
  structure = pdb_parser.get_structure(pdb_filename, pdb_filename)

  # Add the model and set the cartoon style
  viewer = py3Dmol.view(query=f'arch: {name}, pdb: {pdb_filename}', width=width, height=height)
  viewer.addModel(open(pdb_filename, 'r').read(), 'pdb')
  viewer.setStyle({'cartoon': {'color': 'spectrum'}})

  if gaps:
    # Add dashed lines for gaps
    for chain_id, start_res, end_res in gaps:
        try:
            start_residue = structure[0][chain_id][start_res-1]
            end_residue = structure[0][chain_id][end_res]

            # Get coordinates and convert to Python float
            start_coords = [float(coord) for coord in start_residue['CA'].get_coord()]
            end_coords = [float(coord) for coord in end_residue['CA'].get_coord()]

            # Add dashed cylinders for missing residues
            viewer.addCylinder({'start': {'x': start_coords[0], 'y': start_coords[1], 'z': start_coords[2]},
                                'end': {'x': end_coords[0], 'y': end_coords[1], 'z': end_coords[2]},
                                'radius': 0.1, 'color': 'red', 'dashed': True, 'fromCap': 1, 'toCap': 1})
        except KeyError:
          print(f"Residue {start_res} or {end_res} in chain {chain_id} not found.")

  viewer.zoomTo()
  return viewer

import py3Dmol
from IPython.display import display, HTML

pdb_dir = '/content/drive/MyDrive/PDB_files/pdb_share'
num_columns = [2, 3, 5]  # Number of columns in the grid
# titles = ['Structure 1', 'Structure 2', 'Structure 3', 'Structure 4']

# Creating HTML table for the grid
html = '<table><tr>'
print_col = 0
for i, (arch, row) in enumerate(cath_examples.iterrows()):
    # print(row)
    cath_id = row[0]
    pdb_filename = os.path.join(pdb_dir, cath_id)
    if (i-sum(num_columns[:print_col])) % num_columns[print_col] == 0 and i > 0:
        print_col += 1
        html += '</tr><tr>'
    viewer_html = view_structure(pdb_filename, arch)._make_html()
    html += f'<td><div style="text-align: center;"><strong>{arch} {architecture_names[arch]}</strong></div>{viewer_html}</td>'
html += '</tr></table>'

# Display the grid
display(HTML(html))

"""## 3. Note the gaps

Since the protein domain structures are experimental determined, there are some regions of the domain that failed to resolve. These residue show up as gaps in the protein domain sequence and structure.
"""

# Indices of the cath domain associated with the cath_id in the full protein
# that can be found using the pdb id in the PDB online.

example_cath_ids = ['3zq4C03', '3i9v600']
data[data['cath_id'].isin(example_cath_ids)]

# If you compare the indices that in the cath_indices range and not in the PDB file
# residue indices, for these examples you will get this

example_gaps = {'3zq4C03': [('C', 493, 501)],
                '3i9v600': [('6', 58, 74)]}

# We can visualize the gaps are red lines
import py3Dmol
from IPython.display import display, HTML

# Creating HTML table for the grid
html = '<table><tr>'
for cath_id, gap in example_gaps.items():
  viewer_html = view_structure(f'{pdb_dir}/{cath_id}', cath_id, gaps=gap)._make_html()
  html += f'<td><div style="text-align: center;"><strong>{cath_id}</strong></div>{viewer_html}</td>'
html += '</tr></table>'

# Display the grid
display(HTML(html))

"""#Drafting the skeleton"""

from Bio.PDB.Polypeptide import protein_letters_3to1
from Bio.PDB.PDBParser import PDBParser
import torch
import numpy as np


aa_to_index = {aa: idx for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}


def encode_sequence(sequence):
    return torch.tensor([aa_to_index[aa] for aa in sequence if aa in aa_to_index], dtype=torch.long)


# def get_sequence_from_pdb(pdb_filename):
#     pdb_parser = PDBParser()
#     structure = pdb_parser.get_structure(pdb_filename, pdb_filename)
#     assert len(structure) == 1

#     seq = []

#     for model in structure:
#         for chain in model:
#             for residue in chain:
#                 if residue.get_id()[0] == " ":  # This checks if it's a standard residue
#                     seq.append(protein_letters_3to1.get(residue.get_resname(), 'X'))  # 'X' for non-standard
#                 else:
#                     print(f'nonstandard residue at {residue.get_id()}')

#     return ''.join(seq)

pdb_share_directory = '/content/drive/MyDrive/PDB_files/pdb_share'
encoded_sequences = []

# Check if the directory exists
if not os.path.isdir(pdb_share_directory):
    print(f"The directory {pdb_share_directory} does not exist.")
else:
    print(f"Processing files in directory: {pdb_share_directory}")

# Initialize an empty list to store encoded sequences
encoded_sequences = []

# Iterate over each file in the directory and process them
for filename in os.listdir(pdb_share_directory):
    # print(f"Found file: {filename}")  # Print all files found in the directory

    # The complete filepath for each PDB file
    filepath = os.path.join(pdb_share_directory, filename)
    # print(f"Processing PDB file: {filepath}")  # Confirm PDB files are being processed

    # Get the sequence from the PDB file
    sequence = get_sequence_from_pdb(filepath)
    # print(f"Sequence for {filename}: {sequence}")

    # Encode the sequence for model input
    encoded_sequence = encode_sequence(sequence)
    encoded_sequences.append(encoded_sequence)  # Store the encoded sequence
print(encoded_sequences)



"""Transformer model"""

!pip install biopython transformers

from google.colab import drive
drive.mount('/content/drive')

pip install torch torchvision torchaudio

!pip install torch-geometric

import torch
from torch_geometric.data import Data
from Bio.PDB import PDBParser

def structure_to_graph(pdb_filename):
    parser = PDBParser(QUIET=True)  # QUIET=True to avoid unnecessary warnings
    structure = parser.get_structure("PDB_structure", pdb_filename)
    model = next(structure.get_models())  # Assuming we're working with the first model

    # Node features could include more sophisticated properties.
    # Here we'll use a simple feature: the index of the amino acid in the protein sequence.
    nodes = []
    for chain in model:
        for i, res in enumerate(chain.get_residues()):
            if res.get_resname() in Bio.PDB.Polypeptide.standard_aa_names:
                nodes.append(i)  # Using the residue's index as a simple feature

    # Convert to tensor
    node_features = torch.tensor(nodes, dtype=torch.float).unsqueeze(1)  # (num_nodes, num_features)

    # Create edges: for simplicity, connect sequentially adjacent residues
    edge_index = [[], []]
    for i in range(len(nodes) - 1):
        edge_index[0].append(i)
        edge_index[1].append(i + 1)
        edge_index[0].append(i + 1)  # Add both directions for undirected graph
        edge_index[1].append(i)

    edge_index = torch.tensor(edge_index, dtype=torch.long)

    # Creating the graph data object
    data = Data(x=node_features, edge_index=edge_index)
    return data


pdb_filename = '/content/drive/MyDrive/PDB_files/pdb_share'

encoded_structures = []

# Iterate over each file in the directory and process them
for filename in os.listdir(pdb_filename):

    filepath = os.path.join(pdb_filename, filename)

    graph_data = structure_to_graph(filepath)

    encoded_structures.append(graph_data)  # Store the encoded sequence
print(encoded_structures)

len(encoded_structures)
len(encoded_sequences)

from transformers import AutoTokenizer, AutoModel
import torch

from transformers import AutoTokenizer, AutoModel
import torch

def get_sequence_from_pdb_2(pdb_filename):
    pdb_parser = Bio.PDB.PDBParser()
    structure = pdb_parser.get_structure(pdb_filename, pdb_filename)
    assert len(structure) == 1

    seq = []

    for model in structure:
        for chain in model:
            for residue in chain:
                if residue.get_id()[0] == " ":  # This checks if it's a standard residue
                    seq.append(protein_letters_3to1[residue.get_resname()])
                else:
                    print('nonstandard', residue.get_id())

    return ''.join(seq)

# Load ProtBert model
model = AutoModel.from_pretrained("Rostlab/prot_bert")

# Assuming the definition of get_sequence_from_pdb_2 is correct and it extracts the sequences properly

# Custom tokenizer for protein sequences
def custom_tokenize_protein_sequence(sequence, max_length=1000):
    # Map of amino acids to ProtBert token IDs (this is a simplified mapping, you may need to adjust it)
    amino_acid_to_token_id = {
        'A': 4, 'R': 5, 'N': 6, 'D': 7, 'C': 8, 'E': 9, 'Q': 10, 'G': 11, 'H': 12, 'I': 13,
        'L': 14, 'K': 15, 'M': 16, 'F': 17, 'P': 18, 'S': 19, 'T': 20, 'W': 21, 'Y': 22, 'V': 23,
        'X': 1  # Assume 'X' is unknown/any amino acid, mapping to [UNK]
    }
    token_ids = [2] + [amino_acid_to_token_id.get(aa, 1) for aa in sequence][:max_length-2] + [3]
    padding_length = max_length - len(token_ids)
    token_ids += [0] * padding_length  # Padding with [PAD] token
    attention_mask = [1] * (len(token_ids) - padding_length) + [0] * padding_length
    return {"input_ids": torch.tensor([token_ids]), "attention_mask": torch.tensor([attention_mask])}

# Function to encode sequences with ProtBert using custom tokenizer
def encode_sequence_with_protbert_2(sequence, model):
    token = custom_tokenize_protein_sequence(sequence, max_length=1000)
    # print(token)
    with torch.no_grad():
        output = model(**token)
    embeddings = output.last_hidden_state.mean(dim=1)
    return embeddings

encoded_sequences = []

pdb_share_directory = '/content/drive/MyDrive/PDB_files/pdb_share'
# Ensure the directory exists
if not os.path.isdir(pdb_share_directory):
    print(f"The directory {pdb_share_directory} does not exist.")
else:
    # Process each PDB file in the directory
    for filename in os.listdir(pdb_share_directory):
        pdb_filepath = os.path.join(pdb_share_directory, filename)
        sequence = get_sequence_from_pdb_2(pdb_filepath)
        # print(sequence)
        # Skip empty or very short sequences
        if len(sequence) > 10:
            encoded_sequence = encode_sequence_with_protbert_2(sequence, model)
            # print(encoded_sequence)
            encoded_sequences.append(encoded_sequence)
            # print(f"Processed {filename}")
        else:
            print(f"Skipped {filename} due to short or empty sequence")

print(f"Encoded {len(encoded_sequences)} sequences.")

def identify_gaps_sequence(seq_from_csv, seq_from_pdb):
    """Identify gaps based on sequence information alone."""
    # Simple comparison for demonstration purposes. This could be refined.
    gaps = []
    min_len = min(len(seq_from_csv), len(seq_from_pdb))
    for i in range(min_len):
        if seq_from_csv[i] != seq_from_pdb[i]:
            gaps.append(i)
    # Account for length differences
    if len(seq_from_csv) != len(seq_from_pdb):
        gaps.extend(range(min_len, max(len(seq_from_csv), len(seq_from_pdb))))
    return gaps

import pandas as pd

def extract_sequences_from_csv(csv_file_path):
    # Load the CSV file
    df = pd.read_csv(csv_file_path)
    sequences = df.set_index('cath_id').iloc[:, -2].to_dict()#some entries dont have pdb_ids
    return sequences

csv_file_path = '/content/cath_w_seqs_share.csv'
sequences_dict = extract_sequences_from_csv(csv_file_path)
print(sequences_dict)
len(sequences_dict)

import os
from Bio.PDB import PDBParser

def find_structure_gaps(pdb_file_path):
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure('PDB_structure', pdb_file_path)

    gaps = []
    for model in structure:
        for chain in model:
            prev_residue_number = None
            for residue in chain.get_residues():
                if residue.id[0] == ' ':  # Check for hetero/water residues which are not standard amino acids
                    res_id = residue.id[1]
                    if prev_residue_number is not None and res_id - prev_residue_number > 1:
                        # Gap detected
                        gaps.append((prev_residue_number, res_id))
                    prev_residue_number = res_id
    return gaps

# Directory containing PDB files
pdb_files_dir = '/content/drive/MyDrive/PDB_files/pdb_share'

# Dictionary to hold gaps for each PDB file
pdb_gaps = {}

# Loop over all files in the directory (regardless of extension)
for file_name in os.listdir(pdb_files_dir):
    pdb_file_path = os.path.join(pdb_files_dir, file_name)
    if os.path.isfile(pdb_file_path):
        gaps = find_structure_gaps(pdb_file_path)
        pdb_gaps[file_name] = gaps

# Now pdb_gaps dictionary contains all gaps found in each file
for file_name, gaps in pdb_gaps.items():
    if gaps:
        print(f"Gaps found in {file_name}: {gaps}")
    else:
        print(f"No gaps found in {file_name}.")

"""#Identify gaps in sequence and structure"""

pdb_files_dir = '/content/drive/MyDrive/PDB_files/pdb_share'
for pdb_file in os.listdir(pdb_files_dir):
    if pdb_file.endswith(".pdb"):
        pdb_id = pdb_file[:4]  # Adjust based on your PDB file naming convention
        if pdb_id in sequences_dict:
            seq_from_csv = sequences_dict[pdb_id]
            pdb_file_path = os.path.join(pdb_files_dir, pdb_file)
            seq_from_pdb = extract_sequence_from_pdb(pdb_file_path)
            gaps_sequence = identify_gaps_sequence(seq_from_csv, seq_from_pdb)
            gaps_structure = identify_gaps_structure(pdb_file_path)
            print(f"Gaps identified by sequence in {pdb_file}: {gaps_sequence}")
            print(f"Gaps identified by structure in {pdb_file}: {gaps_structure}")

!pip install biopython

import pandas as pd
import os
from Bio.PDB import PDBParser
from Bio.SeqUtils import seq1

def load_sequences_from_csv(csv_file_path):
    """
    Load sequences from a CSV file into a dictionary.
    """
    df = pd.read_csv(csv_file_path)
    sequences = df.set_index('cath_id')['sequences'].to_dict()
    return sequences

def load_pdb_structures(pdb_directory):
    """
    Load PDB structures from files in a directory. This version does not filter files by the '.pdb' extension.
    """
    pdb_files = [file for file in os.listdir(pdb_directory)]  # Removed the condition to filter by '.pdb'
    structures = {}
    for file in pdb_files:
        cath_id = file
        # print(f"Processing {file} as {cath_id}...")  # Debugging
        pdb_file_path = os.path.join(pdb_directory, file)
        if os.path.isfile(pdb_file_path):  # Check if it's a file
            structure = parse_pdb_structure(pdb_file_path)
            if structure:  # Check if structure is not None
                structures[cath_id] = structure
            else:
                print(f"Failed to parse structure from {file}.")  # Debugging
        else:
            print(f"Skipping {file}, not a file.")  # Handle cases where directory entries might not be files
    return structures


def parse_pdb_structure(pdb_file_path):
    """
    Parse a PDB file to extract structure features.
    """
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure('PDB_structure', pdb_file_path)
    structure_features = {'residues': [], 'atoms': []}

    for model in structure:
        for chain in model:
            for residue in chain:
                if residue.id[0] == ' ':  # Filter out hetero atoms.
                    residue_features = extract_residue_features(residue)
                    structure_features['residues'].append(residue_features)
                    for atom in residue:
                        atom_features = extract_atom_features(atom)
                        structure_features['atoms'].append(atom_features)
    return structure_features

def extract_residue_features(residue):
    """
    Extract relevant features from a residue.
    """
    residue_features = {
        'residue_name': residue.get_resname(),
        'residue_number': residue.id[1],
        'chain_id': residue.parent.id
    }
    return residue_features

def extract_atom_features(atom):
    """
    Extract relevant features from an atom.
    """
    atom_features = {
        'atom_name': atom.get_name(),
        'atom_element': atom.element,
        'atom_coords': atom.get_coord()
    }
    return atom_features

csv_file_path = '/content/cath_w_seqs_share.csv'
pdb_directory = '/content/drive/MyDrive/PDB_files/pdb_share'

# Load sequences and structures using cath_id
# sequences = load_sequences_from_csv(csv_file_path)
# structures = load_pdb_structures(pdb_directory)
# print('sequences:', sequences)
# print('structures:', structures)
# Now you have sequences and structures loaded based on cath_id
# Proceed with further analysis, feature extraction, or model training as needed

for key,item in structures.items():
  print(key, item)

def identify_gaps_in_sequence(seq_from_csv, seq_from_pdb):
    gaps = []
    min_len = min(len(seq_from_csv), len(seq_from_pdb))
    for i in range(min_len):
        if seq_from_csv[i] != seq_from_pdb[i]:
            gaps.append(i)
    if len(seq_from_csv) != len(seq_from_pdb):
        gaps.extend(range(min_len, max(len(seq_from_csv), len(seq_from_pdb))))
    return gaps


def identify_gaps_in_structure_efficient(pdb_structure):
    gaps = []
    for chain in pdb_structure.get_chains():
        prev_residue_number = None
        for residue in chain:
            if residue.id[0] == ' ':  # Filter standard residues
                res_id = residue.id[1] if isinstance(residue.id[1], int) else residue.id[1][0]  # Adjust for insertion codes
                if prev_residue_number and res_id - prev_residue_number > 1:
                    gaps.append((prev_residue_number + 1, res_id - 1))
                prev_residue_number = res_id
    return gaps

"""#for 150 files

#Now identify the gaps in the sequence file and in the structure file

Then mask it.
"""

# import pandas as pd
# import requests
# from Bio.PDB import PDBParser
# from Bio.SeqUtils import seq1

# def fetch_uniprot_sequence(uniprot_id):
#     """
#     Fetch the sequence for a given UniProt ID from UniProt.
#     """
#     url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta"
#     response = requests.get(url)
#     if response.status_code == 200:
#         # Extract sequence from FASTA
#         sequence = ''.join(response.text.split('\n')[1:])
#         return sequence
#     else:
#         return None

# def compare_sequences(seq1, seq2):
#     """
#     Identifies differences between two sequences and returns the positions of gaps.
#     """
#     gaps = []
#     min_len = min(len(seq1), len(seq2))
#     for i in range(min_len):
#         if seq1[i] != seq2[i]:
#             gaps.append(i)
#     if len(seq1) != len(seq2):
#         gaps.extend(range(min_len, max(len(seq1), len(seq2))))
#     return gaps

# def identify_structure_gaps(pdb_file_path):
#     """
#     Identifies gaps in the structure by checking discontinuities in residue numbering.
#     """
#     parser = PDBParser(QUIET=True)
#     structure = parser.get_structure('PDB_structure', pdb_file_path)
#     gaps = []
#     for chain in structure.get_chains():
#         prev_residue_number = None
#         for residue in chain:
#             if residue.id[0] == ' ':
#                 res_id = residue.id[1] if isinstance(residue.id[1], int) else residue.id[1][0]
#                 if prev_residue_number and res_id - prev_residue_number > 1:
#                     gaps.append((prev_residue_number + 1, res_id - 1))
#                 prev_residue_number = res_id
#     return gaps

# # Example usage
# csv_file_path = 'path/to/your/csv_file.csv'
# df = pd.read_csv(csv_file_path)

# for index, row in df.iterrows():
#     uniprot_id = row['uniprot_id']  # Adjust column name as necessary
#     cath_id = row['cath_id']  # Adjust column name as necessary
#     pdb_file_path = f'path/to/pdb/files/{cath_id}.pdb'  # Adjust path and file naming convention as necessary

#     uniprot_sequence = fetch_uniprot_sequence(uniprot_id)
#     pdb_sequence = parse_pdb_sequence(pdb_file_path)  # Function to parse sequence from PDB needs to be implemented

#     if uniprot_sequence and pdb_sequence:
#         seq_gaps = compare_sequences(uniprot_sequence, pdb_sequence)
#         struct_gaps = identify_structure_gaps(pdb_file_path)

#         print(f"{cath_id} - Sequence Gaps: {seq_gaps}, Structure Gaps: {struct_gaps}")
#         # Further processing for masking or averaging

import requests

def fetch_uniprot_id_from_pdb(pdb_id):
    # Placeholder URL - Replace with the actual API endpoint that can provide UniProt ID mappings
    url = f"https://example.com/api/mapping/pdb_to_uniprot/{pdb_id}"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        # Assuming the API returns a JSON object with a 'uniprot_id' field
        uniprot_id = data['uniprot_id']
        return uniprot_id
    else:
        print(f"Failed to fetch UniProt ID for PDB ID {pdb_id}. Status code: {response.status_code}")
        return None

import requests

def fetch_reference_sequence_from_uniprot(uniprot_id):
    """
    Fetch the protein sequence for a given UniProt ID using the UniProt API.
    """
    url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta"
    response = requests.get(url)
    if response.status_code == 200:
        # Extract sequence from FASTA format
        fasta_data = response.text
        lines = fasta_data.split('\n')
        sequence = ''.join(lines[1:])  # Skip the first line which is the header
        return sequence
    else:
        print(f"Failed to fetch sequence for UniProt ID {uniprot_id}. Status code: {response.status_code}")
        return None

# Example usage
uniprot_id = "P00734"  # Example UniProt ID
sequence = fetch_reference_sequence_from_uniprot(uniprot_id)
print(f"Sequence for UniProt ID {uniprot_id}:", sequence)

"""1. Create a function to read the first 150 cath_ids and their corresponding pdb_ids from the CSV.
2. Fetch UniProt sequences for each pdb_id and map them to cath_id.
3. Compare these sequences against your provided sequences to identify gaps.
4. Mask the gaps appropriately.
5. Provide the final sequence dictionary ready for input into a transformer.
"""

import pandas as pd
import requests

def fetch_uniprot_sequence(pdb_id):
    """
    Fetch the protein sequence from UniProt given a PDB ID.
    This function assumes a successful PDB to UniProt mapping exists.
    """
    mapping_url = f"https://www.ebi.ac.uk/pdbe/api/pdb/entry/mappings/{pdb_id}"
    mapping_response = requests.get(mapping_url)
    if mapping_response.status_code != 200:
        return None

    mapping_data = mapping_response.json()
    try:
        uniprot_id = mapping_data[pdb_id][0]['uniprot_acc']
        sequence_url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta"
        sequence_response = requests.get(sequence_url)
        if sequence_response.status_code == 200:
            fasta_data = sequence_response.text.split('\n', 1)[1].replace('\n', '')
            return fasta_data
    except (IndexError, KeyError):
        return None

def create_cath_to_uniprot_map(csv_file_path, limit=150):
    """
    Reads the first 'limit' cath_ids from a CSV file and fetches corresponding UniProt sequences.
    Returns a dictionary mapping cath_id to UniProt sequences.
    """
    df = pd.read_csv(csv_file_path)
    cath_to_uniprot = {}

    for _, row in df.head(limit).iterrows():
        cath_id, pdb_id = row['cath_id'], row['pdb_id']
        uniprot_sequence = fetch_uniprot_sequence(pdb_id)
        if uniprot_sequence:
            cath_to_uniprot[cath_id] = uniprot_sequence
        else:
            continue #print(f"UniProt sequence not found for PDB ID: {pdb_id}")

    return cath_to_uniprot

cath_to_uniprot_sequences = create_cath_to_uniprot_map('/content/cath_w_seqs_share.csv')

# Step 2: Identify gaps and mask sequences
def identify_and_mask_gaps(cath_id_sequences, uniprot_sequences):
    """
    Identifies gaps and mismatches between sequences from the dataset (cath_id_sequences)
    and sequences fetched from UniProt (uniprot_sequences), then masks these discrepancies.

    Parameters:
    - cath_id_sequences: dict, where keys are cath_ids and values are sequences from your dataset.
    - uniprot_sequences: dict, where keys are cath_ids and values are sequences from UniProt.

    Returns:
    - masked_sequences: dict, with cath_ids as keys and masked sequences as values.
    """
    masked_sequences = {}

    for cath_id, cath_seq in cath_id_sequences.items():
        uniprot_seq = uniprot_sequences.get(cath_id, "")

        # Initialize variables to build the masked sequence
        masked_seq = []
        cath_seq_len = len(cath_seq)
        uniprot_seq_len = len(uniprot_seq)
        max_len = max(cath_seq_len, uniprot_seq_len)

        # Loop through the maximum length of the two sequences
        for i in range(max_len):
            cath_char = cath_seq[i] if i < cath_seq_len else None
            uniprot_char = uniprot_seq[i] if i < uniprot_seq_len else None

            # If characters match or UniProt sequence is longer (cath_char is None), keep the character
            if cath_char == uniprot_char or cath_char is None:
                masked_seq.append(uniprot_char if uniprot_char is not None else '<mask>')
            # If characters do not match or cath_id sequence is longer (uniprot_char is None), mask the character
            else:
                masked_seq.append('<mask>')

        # Join the list into a string to form the final masked sequence
        masked_sequences[cath_id] = ''.join(masked_seq)

    return masked_sequences


# masked_sequences = identify_and_mask_gaps(cath_id_sequences, uniprot_sequences)

# for cath_id, seq in masked_sequences.items():
#     print(f"{cath_id}: {seq}")

def load_csv_and_fetch_sequences(csv_file_path, existing_cath_id_sequences):
    df = pd.read_csv(csv_file_path)
    failed_pdb_ids = {}  # Track PDB IDs for which fetching UniProt ID or sequence fails

    cath_id_to_sequence = {}
    for index, row in df.iterrows():
        cath_id = row['cath_id']
        pdb_id = row['pdb_id']

        # Fetch UniProt ID
        uniprot_id = fetch_uniprot_id_from_pdb(pdb_id)
        if uniprot_id:
            # Fetch sequence from UniProt
            sequence = fetch_reference_sequence_from_uniprot(uniprot_id)
            if sequence:
                cath_id_to_sequence[cath_id] = sequence
            else:
                # Use the cath_id sequence from existing dictionary
                print(f"Failed to fetch sequence for UniProt ID {uniprot_id}. Using cath_id sequence.")
                cath_id_to_sequence[cath_id] = existing_cath_id_sequences.get(cath_id, "")
                failed_pdb_ids[pdb_id] = "Failed to fetch sequence"
        else:
            print(f"Failed to fetch UniProt ID for PDB ID {pdb_id}. Using cath_id sequence.")
            cath_id_to_sequence[cath_id] = existing_cath_id_sequences.get(cath_id, "")
            failed_pdb_ids[pdb_id] = "Failed to fetch UniProt ID"

    return cath_id_to_sequence, failed_pdb_ids

# Assuming you have the path to your CSV and existing sequences dictionary
csv_file_path = '/content/cath_w_seqs_share.csv'
existing_cath_id_sequences = {
    # Your existing sequences, for example: 'cath_id1': 'MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDRFKHLKTEAEMKASEDLKKHGVTVLTALGAILKKKGHHEAELKPLAQSHATKHKIPIKYLEFISEAIIHQLQSKHPGDFGADAQGAMNKALELFRKDIAAKYKELGYQG',
}

# Fetch sequences and track failed PDB IDs
cath_id_to_sequence, failed_pdb_ids = load_csv_and_fetch_sequences(csv_file_path, existing_cath_id_sequences)

# Print failed PDB IDs for debugging or further action
print("Failed PDB IDs:", failed_pdb_ids)

"""#Fetching homology sequence data from Uniprot

---


"""

import requests
import time

def submit_mapping_job(pdb_ids):
    """Submits a mapping job to UniProt to convert PDB IDs to UniProt IDs."""
    url = "https://rest.uniprot.org/idmapping/run"
    data = {
        'from': 'PDB',
        'to': 'UniProtKB',
        'ids': pdb_ids
    }
    response = requests.post(url, data=data)
    response.raise_for_status()
    # Assuming the job ID is directly returned; adjust based on actual API response
    return response.json()['jobId']

def get_mapping_results(job_id):
    """Polls for and retrieves the results of a mapping job from UniProt."""
    results_url = f"https://rest.uniprot.org/idmapping/status/{job_id}"
    while True:
        response = requests.get(results_url)
        if response.status_code == 200:
            result_data = response.json()
            # Check if the job is finished; adjust based on actual API response
            if result_data.get('status') == 'FINISHED':
                # Fetch the actual results; adjust the URL based on actual API response
                return requests.get(result_data['resultsUrl']).json()
            elif result_data.get('status') == 'ERROR':
                # Handle job error
                raise Exception("Error in processing the mapping job.")
        time.sleep(5)  # Adjust polling interval as needed

def map_pdb_to_uniprot(pdb_ids):
    """Maps PDB IDs to UniProt IDs by submitting a job and fetching the results."""
    job_id = submit_mapping_job(pdb_ids)
    results = get_mapping_results(job_id)
    # Process the results to extract UniProt IDs; adjust based on actual results format
    pdb_to_uniprot = {result['pdbId']: result['uniProtId'] for result in results}
    return pdb_to_uniprot

# Example usage
pdb_ids = ["4HHB", "1A02"]
try:
    pdb_to_uniprot = map_pdb_to_uniprot(pdb_ids)
    for pdb_id, uniprot_id in pdb_to_uniprot.items():
        print(f"PDB ID {pdb_id} maps to UniProt ID {uniprot_id}")
except Exception as e:
    print(f"An error occurred: {e}")

def mask_structure_gaps(structure_dict):
    """
    Identify gaps in the structure based on residue numbering and return a dictionary with masked structures.

    Parameters:
    structure_dict (dict): Dictionary containing structure information for each cath_id.

    Returns:
    dict: A dictionary with cath_ids as keys and masked structure lists as values.
    """
    masked_structures = {}

    for cath_id, structure_info in structure_dict.items():
        residues = structure_info['residues']
        masked_residues = []
        prev_residue_num = None

        for residue in residues:
            residue_num = residue['residue_number']

            # Check for a gap
            if prev_residue_num is not None and residue_num - prev_residue_num > 1:
                # Calculate gap size
                gap_size = residue_num - prev_residue_num - 1
                # Append masked residues for the gap
                masked_residues.extend([{'residue_name': 'MASK', 'residue_number': i, 'chain_id': residue['chain_id']} for i in range(prev_residue_num + 1, residue_num)])

            masked_residues.append(residue)
            prev_residue_num = residue_num

        masked_structures[cath_id] = {'residues': masked_residues}

    return masked_structures

masked_structures = mask_structure_gaps(structure_dict)
print(masked_structures)

import numpy as np

def encode_structural_context(structures_dict, gap_feature_size=5):
    """
    Encodes the structural context of gaps in protein structures, adding features to gap nodes.

    Parameters:
    - structures_dict: dict, containing parsed structure information for proteins.
    - gap_feature_size: int, the size of the feature vector to represent each gap.

    Returns:
    - encoded_structures: dict, with the same structure as structures_dict but with added gap features.
    """
    encoded_structures = {}

    for cath_id, structure in structures_dict.items():
        # Initialize the list to hold encoded residues including gaps
        encoded_residues = []

        residues = structure['residues']
        prev_residue_number = None
        for residue in residues:
            # If the residue is a gap, encode its context
            if residue['residue_name'] == 'MASK':
                gap_features = np.zeros(gap_feature_size)
                # Example encoding strategy: Use the position in the sequence as a feature
                if prev_residue_number is not None:
                    position_in_sequence = (residue['residue_number'] - prev_residue_number) / len(residues)
                    gap_features[0] = position_in_sequence
                # Add more context features as needed
                encoded_residues.append({'residue_name': 'MASK', 'features': gap_features})
            else:
                # For standard residues, you could also add features, e.g., one-hot encoding of residue names
                encoded_residues.append(residue)  # Placeholder for actual residue feature encoding
            prev_residue_number = residue['residue_number']

        # Save the encoded structure back to the dictionary
        encoded_structures[cath_id] = {'residues': encoded_residues}

    return encoded_structures

encoded_structures = encode_structural_context(structures)
print(encoded_structures)
print(len(encoded_structures))

pip install torch-geometric

import torch
from torch_geometric.data import Data
import numpy as np

def structure_dict_to_graph(structure_dict):
    """
    Converts a parsed structure dictionary into a graph representation.

    Args:
    - structure_dict: dict, The structure dictionary where keys are cath_ids and
      values are dictionaries containing 'residues' and 'atoms'.

    Returns:
    - graph_data: dict, A dictionary of PyTorch Geometric Data objects, keyed by cath_id.
    """
    graph_data = {}

    for cath_id, structure in structure_dict.items():
        # Nodes represent residues; node features could include residue type encoded as integers
        node_features = []
        for residue in structure['residues']:
            residue_name = residue['residue_name']
            # Example: encode residue names into integers (implement your own encoding)
            node_features.append(residue_name_to_int(residue_name))

        # Convert to tensor
        node_features_tensor = torch.tensor(node_features, dtype=torch.float).view(-1, 1)

        # Edges: connect sequentially adjacent residues as a simple example
        edge_index = [[], []]
        for i in range(len(structure['residues']) - 1):
            edge_index[0].append(i)
            edge_index[1].append(i + 1)
            # For undirected graphs, add edges in both directions
            edge_index[0].append(i + 1)
            edge_index[1].append(i)

        # Convert to tensor
        edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)

        # Create PyTorch Geometric data object
        data = Data(x=node_features_tensor, edge_index=edge_index_tensor)
        graph_data[cath_id] = data

    return graph_data


def residue_name_to_int(residue_name):
    """
    Encodes residue names to integers, covering all 20 standard amino acids.
    """
    mapping = {
        'ALA': 1, 'CYS': 2, 'ASP': 3, 'GLU': 4,
        'PHE': 5, 'GLY': 6, 'HIS': 7, 'ILE': 8,
        'LYS': 9, 'LEU': 10, 'MET': 11, 'ASN': 12,
        'PRO': 13, 'GLN': 14, 'ARG': 15, 'SER': 16,
        'THR': 17, 'VAL': 18, 'TRP': 19, 'TYR': 20,
        'MASK': 0, 'UNK': 0  # Placeholder for gaps
    }
    return mapping.get(residue_name, -1)  # Return -1 for unknown residues

# graph_data = structure_dict_to_graph(encoded_structures)
# print(graph_data)

import sys
if not os.path.isdir("ProteinMPNN"):
  !git clone -q https://github.com/dauparas/ProteinMPNN.git
sys.path.append('/content/ProteinMPNN')

!pip install torch-geometric

# If necessary, install py3Dmol
!pip install py3Dmol

import py3Dmol
from IPython.display import display, HTML
import matplotlib.pyplot as plt
import os
import pandas as pd

# Assuming you have a DataFrame with cath_ids and labels
df = pd.read_csv('/content/cath_w_seqs_share.csv')
pdb_dir = '/content/drive/MyDrive/PDB_files/pdb_share'
# Directory to save images
image_dir = '/content/ImageFiles'
if not os.path.exists(image_dir):
    os.makedirs(image_dir)

for index, row in df.iterrows():
    cath_id = row['cath_id']
    label = row['architecture']  # Ensure you have labels in your DataFrame
    pdb_filename = os.path.join(pdb_dir, f"{cath_id}")  # Adjust the path and file extension as necessary
    image_filename = os.path.join(image_dir, f"{cath_id}.png")
    save_structure_as_image(pdb_filename, image_filename, view_options={'stick': {}})

import pymol
pymol.pymol_argv = ['pymol', '-cQi']

"""#Attempt to Convert protein structures to images and train a CNN or GCN"""

!apt-get install -y python3-pymol

!apt-get install -y xvfb

import os
os.system('Xvfb :99 &')
os.environ['DISPLAY'] = ':99'

# Install necessary tools and PyMOL in Google Colab
!apt-get install -y xvfb python-opengl > /dev/null 2>&1
!pip install pymol pyvirtualdisplay > /dev/null 2>&1

import os
from pathlib import Path
from pyvirtualdisplay import Display

# Start a virtual display
display = Display(visible=0, size=(800, 600))
display.start()

# Import PyMOL
from pymol import cmd


pdb_dir_path = Path('/content/drive/MyDrive/PDB_files/pdb_share')

# List all PDB files
pdb_files = list(pdb_dir_path.glob('*.pdb'))

# Select the first 15 PDB files
selected_pdb_files = pdb_files[:15]

# Function to render and save structure images with enhanced styling
def render_structure_enhanced(pdb_path, output_image_path):
    cmd.reinitialize()  # Reinitialize PyMOL session for each structure
    pdb_name = pdb_path.stem
    cmd.load(str(pdb_path), pdb_name)
    cmd.disable("all")
    cmd.enable(pdb_name)
    cmd.hide('all')
    cmd.show('cartoon')
    cmd.set('ray_opaque_background', 0)
    cmd.color('red', 'ss h')  # Color helices red
    cmd.color('yellow', 'ss s')  # Color sheets yellow
    cmd.ray(800, 600)  # Set image size
    cmd.png(str(output_image_path))  # Save the image

# Directory to save the images
image_dir = Path('/content/Image_file')
image_dir.mkdir(exist_ok=True)

# Process the selected PDB files
for pdb_path in selected_pdb_files:
    output_image_name = pdb_path.stem + '.png'  # Use stem to get the file name without extension
    output_image_path = image_dir / output_image_name
    print(f"Rendering: {pdb_path} to {output_image_path}")
    render_structure_enhanced(pdb_path, output_image_path)

print('Done rendering PDB structures with enhanced styling.')

"""get sequence from pdb file, 150"""

import pandas as pd
import os
from Bio.PDB import PDBParser
from Bio.SeqUtils import seq1

def load_sequences_from_csv(csv_file_path, limit=150):
    df = pd.read_csv(csv_file_path)
    sequences = df.set_index('cath_id')['sequences'].to_dict()
    return dict(list(sequences.items())[:limit])

def load_pdb_structures(pdb_directory, limit=150):
    pdb_files = sorted([f for f in os.listdir(pdb_directory)])[:limit]
    structures = {}
    for pdb_file in pdb_files:
        cath_id = pdb_file.split('.')[0]  # Assuming the filename format is 'cath_id.pdb'
        pdb_file_path = os.path.join(pdb_directory, pdb_file)
        structures[cath_id] = parse_pdb_structure(pdb_file_path)
    return structures

def parse_pdb_structure(pdb_file_path):
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure('', pdb_file_path)  # Second argument is an ID for the structure
    structure_features = {'residues': [], 'atoms': []}

    for model in structure:
        for chain in model:
            for residue in chain:
                if residue.id[0] == ' ':
                    residue_features = {
                        'residue_name': residue.get_resname(),
                        'residue_number': residue.id[1],
                        'chain_id': chain.id
                    }
                    structure_features['residues'].append(residue_features)

                    for atom in residue:
                        atom_features = {
                            'atom_name': atom.get_name(),
                            'atom_element': atom.element,
                            'atom_coords': atom.get_coord()
                        }
                        structure_features['atoms'].append(atom_features)
    return structure_features

# Update these paths to match your file locations
csv_file_path = '/content/cath_w_seqs_share.csv'
pdb_directory = '/content/drive/MyDrive/PDB_files/pdb_share'

# sequences = load_sequences_from_csv(csv_file_path)
# # structures = load_pdb_structures(pdb_directory)

# print('Loaded Sequences:', len(sequences))
# # print('Loaded Structures:', len(structures))
# import copy
# orginial_sequences = copy.deepcopy(sequences)

"""#Attempt to implement model ProtienMPNN (which takes both sequence and graph structure) for calssification"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/dauparas/ProteinMPNN.git
# %cd /content/

import importlib.util
import sys

# Specify the path to protein_mpnn_utils.py
module_path = '/content/ProteinMPNN/protein_mpnn_utils.py'

# Load the module
spec = importlib.util.spec_from_file_location("protein_mpnn_utils", module_path)
protein_mpnn_utils = importlib.util.module_from_spec(spec)
sys.modules["protein_mpnn_utils"] = protein_mpnn_utils
spec.loader.exec_module(protein_mpnn_utils)

import torch
from torch_geometric.data import Data
import numpy as np

def structure_dict_to_graph(structure_dict):
    """
    Converts a parsed structure dictionary into a graph representation.

    Args:
    - structure_dict: dict, The structure dictionary where keys are cath_ids and
      values are dictionaries containing 'residues' and 'atoms'.

    Returns:
    - graph_data: dict, A dictionary of PyTorch Geometric Data objects, keyed by cath_id.
    """
    graph_data = {}

    for cath_id, structure in structure_dict.items():
        # Nodes represent residues; node features could include residue type encoded as integers
        node_features = []
        for residue in structure['residues']:
            residue_name = residue['residue_name']
            # Example: encode residue names into integers (implement your own encoding)
            node_features.append(residue_name_to_int(residue_name))

        # Convert to tensor
        node_features_tensor = torch.tensor(node_features, dtype=torch.float).view(-1, 1)

        # Edges: connect sequentially adjacent residues as a simple example
        edge_index = [[], []]
        for i in range(len(structure['residues']) - 1):
            edge_index[0].append(i)
            edge_index[1].append(i + 1)
            # For undirected graphs, add edges in both directions
            edge_index[0].append(i + 1)
            edge_index[1].append(i)

        # Convert to tensor
        edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)

        # Create PyTorch Geometric data object
        data = Data(x=node_features_tensor, edge_index=edge_index_tensor)
        graph_data[cath_id] = data

    return graph_data


def residue_name_to_int(residue_name):
    """
    Encodes residue names to integers, covering all 20 standard amino acids.
    """
    mapping = {
        'ALA': 1, 'CYS': 2, 'ASP': 3, 'GLU': 4,
        'PHE': 5, 'GLY': 6, 'HIS': 7, 'ILE': 8,
        'LYS': 9, 'LEU': 10, 'MET': 11, 'ASN': 12,
        'PRO': 13, 'GLN': 14, 'ARG': 15, 'SER': 16,
        'THR': 17, 'VAL': 18, 'TRP': 19, 'TYR': 20,
        'MASK': 0, 'UNK': 0  # Placeholder for gaps
    }
    return mapping.get(residue_name, -1)  # Return -1 for unknown residues

# graph_data = structure_dict_to_graph(encoded_structures)
# print(graph_data)

"""prepare train test split"""

import pandas as pd
from sklearn.model_selection import train_test_split


df = pd.read_csv('/content/cath_w_seqs_share.csv')


df['label'] = df['architecture']

# Group by 'superfamily' to ensure related proteins are in the same split
grouped = df.groupby('superfamily')

train = []
val = []
test = []

# Modify label lists to store tuples of (cath_id, label)
train_labels = []
val_labels = []
test_labels = []

for _, group in grouped:
    # Check the size of the group
    group_size = len(group)
    if group_size > 2:
        # Enough samples for splitting
        train_split, temp_split = train_test_split(group, test_size=0.4, random_state=42)
        if len(temp_split) > 1:
            val_split, test_split = train_test_split(temp_split, test_size=0.5, random_state=42)
        else:

            val_split = temp_split
            test_split = pd.DataFrame()
    elif group_size == 2:
        # Only two samples, split into train and val
        train_split, val_split = train_test_split(group, test_size=0.5, random_state=42)
        test_split = pd.DataFrame()
    else:
        # Single sample, put it into train
        train_split = group
        val_split = pd.DataFrame()
        test_split = pd.DataFrame()

    train.append(train_split)
    val.append(val_split)
    test.append(test_split)

    # Append tuples of (cath_id, label) for each split
    if not train_split.empty:
        train_labels.extend(zip(train_split['cath_id'], train_split['label']))
    if not val_split.empty:
        val_labels.extend(zip(val_split['cath_id'], val_split['label']))
    if not test_split.empty:
        test_labels.extend(zip(test_split['cath_id'], test_split['label']))

train_df = pd.concat(train, ignore_index=True) if train else pd.DataFrame()
val_df = pd.concat(val, ignore_index=True) if val else pd.DataFrame()
test_df = pd.concat(test, ignore_index=True) if test else pd.DataFrame()

# Convert label lists to DataFrame for easier manipulation and access
train_labels_df = pd.DataFrame(train_labels, columns=['cath_id', 'label'])
val_labels_df = pd.DataFrame(val_labels, columns=['cath_id', 'label'])
test_labels_df = pd.DataFrame(test_labels, columns=['cath_id', 'label'])

# Output the datasets and their labels
print("Training set:\n", train_df.head())
print("Training labels:\n", train_labels_df.head())
print("Validation set:\n", val_df.head())
print("Validation labels:\n", val_labels_df.head())
print("Test set:\n", test_df.head())
print("Test labels:\n", test_labels_df.head())

"""Loading certain structural data from the train_df"""

import pandas as pd
import os
from pathlib import Path

# Specify the directory where your PDB files are stored
pdb_directory_path = '/content/drive/MyDrive/PDB_files/pdb_share'

pdb_directory = Path(pdb_directory_path)

# Initialize an empty dictionary to store PDB file paths
pdb_file_paths_train = {}


for cath_id in train_df['cath_id']:

    pdb_file_path = pdb_directory / f'{cath_id}'

    # Check if the file exists
    if pdb_file_path.is_file():
        # If the file exists, store the path in the dictionary
        pdb_file_paths_train[cath_id] = pdb_file_path
    else:
        # If the file does not exist, print a message or handle it as you see fit
        print(f'PDB file for cath_id {cath_id} not found.')

from Bio.PDB import PDBParser, PDBIO, Select
import numpy as np
import networkx as nx
from tqdm import tqdm

def parse_pdb_structure(pdb_file_path):
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure('', pdb_file_path)  # Second argument is an ID for the structure
    structure_features = {'residues': [], 'atoms': []}

    for model in structure:
        for chain in model:
            for residue in chain:
                if residue.id[0] == ' ':
                    residue_features = {
                        'residue_name': residue.get_resname(),
                        'residue_number': residue.id[1],
                        'chain_id': chain.id
                    }
                    structure_features['residues'].append(residue_features)

                    for atom in residue:
                        atom_features = {
                            'atom_name': atom.get_name(),
                            'atom_element': atom.element,
                            'atom_coords': atom.get_coord()
                        }
                        structure_features['atoms'].append(atom_features)
    return structure_features

def load_and_center_pdb_structure(pdb_file_path):
    parser = PDBParser(QUIET=True)
    structure_id = os.path.basename(pdb_file_path).split('.')[0]
    structure = parser.get_structure(structure_id, pdb_file_path)

    # Centralize structure
    atoms = list(structure.get_atoms())
    atom_coords = [atom.coord for atom in atoms]
    centroid = np.mean(atom_coords, axis=0)
    for atom in atoms:
        atom.coord -= centroid

    # Parse structure into features
    structure_features = parse_pdb_structure(pdb_file_path)
    return structure_features



# Function to convert centralized structures to graph representations
import torch
from torch_geometric.data import Data
import numpy as np

def structure_dict_to_graph(structure_dict):

    graph_data = {}

    for cath_id, structure in structure_dict.items():
        node_features = []
        for residue in structure['residues']:
            residue_name = residue['residue_name']
            node_features.append(residue_name_to_int(residue_name))

        # Convert to tensor
        node_features_tensor = torch.tensor(node_features, dtype=torch.float).view(-1, 1)

        edge_index = [[], []]
        for i in range(len(structure['residues']) - 1):
            edge_index[0].append(i)
            edge_index[1].append(i + 1)
            # For undirected graphs, add edges in both directions
            edge_index[0].append(i + 1)
            edge_index[1].append(i)

        # Convert to tensor
        edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)

        # Create PyTorch Geometric data object
        data = Data(x=node_features_tensor, edge_index=edge_index_tensor)
        graph_data[cath_id] = data

    return graph_data

def residue_name_to_int(residue_name):
    """
    Encodes residue names to integers, covering all 20 standard amino acids.
    """
    mapping = {
        'ALA': 1, 'CYS': 2, 'ASP': 3, 'GLU': 4,
        'PHE': 5, 'GLY': 6, 'HIS': 7, 'ILE': 8,
        'LYS': 9, 'LEU': 10, 'MET': 11, 'ASN': 12,
        'PRO': 13, 'GLN': 14, 'ARG': 15, 'SER': 16,
        'THR': 17, 'VAL': 18, 'TRP': 19, 'TYR': 20,
        'MASK': 0, 'UNK': 0  # Placeholder for gaps
    }
    return mapping.get(residue_name, -1)  # Return -1 for unknown residues

# graph_data = structure_dict_to_graph(encoded_structures)
# print(graph_data)


structure_graphs = {}

for cath_id, pdb_file_path in tqdm(pdb_file_paths_train.items()):

    centered_structure = load_and_center_pdb_structure(pdb_file_path)
    graph = structure_dict_to_graph({cath_id: centered_structure})
    structure_graphs[cath_id] = graph

# Now `structure_graphs` contains the graph representations of all structures

structure_graphs

"""#Formatting data to load it

"""

import pandas as pd
import torch
from torch_geometric.data import Dataset, Data, DataLoader

class ProteinDataset(Dataset):
    def __init__(self, protein_graph_list, labels, csv_file, transform=None, pre_transform=None):
        super(ProteinDataset, self).__init__(None, transform, pre_transform)
        self.protein_graph_list = protein_graph_list
        self.labels = torch.tensor(labels, dtype=torch.long)
        print(labels)
        print(protein_graph_list)
        self.sequence_dict = self.load_sequences_from_csv(csv_file)

    def load_sequences_from_csv(self, csv_file):
        df = pd.read_csv(csv_file)
        sequence_dict = pd.Series(df['sequences'].values, index=df['cath_id']).to_dict()
        print(sequence_dict)
        return sequence_dict

    def len(self):
        return len(self.protein_graph_list)

    def get(self, idx):
        data_obj = self.protein_graph_list[idx]

        # Ensure data_obj is indeed a Data object
        if not isinstance(data_obj, Data):
            raise TypeError(f"Expected PyTorch Geometric Data object, got {type(data_obj)}")

        try:
            cath_id = data_obj.cath_id  # Attempt to access cath_id
        except AttributeError:
            raise AttributeError(f"'Data' object at index {idx} lacks 'cath_id' attribute")

        sequence = self.sequence_dict.get(cath_id, "")
        y_sequence = torch.tensor([self.sequence_to_label(aa) for aa in sequence], dtype=torch.long)

        # Set y_sequence as an attribute; adjust this according to your requirements
        data_obj.y_sequence = y_sequence  # Assuming you want to keep original labels in .y

        return data_obj

    @staticmethod
    def sequence_to_label(aa):
        label_dict = {
            'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9,
            'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19,
            'X': 20  # 'X' as unknown/any amino acid
        }
        return label_dict.get(aa, 20)  # Return index for 'X' if aa is not found


# Usage
csv_file = '/content/cath_w_seqs_share.csv'  # Adjust to your CSV file's path


# Convert items in protein_graph_list to Data objects if they are not already
protein_graph_list = [dict_to_data_object(item) if isinstance(item, dict) else item for item in protein_graph_list]


cath_ids = train_labels['cath_id'].tolist()  # Keep cath_ids as a list for reference
numerical_labels = train_labels['label'].values

protein_dataset = ProteinDataset(protein_graph_list, train_labels, csv_file)

protein_data_loader = DataLoader(protein_dataset, batch_size=32, shuffle=True)

# Iterating over DataLoader in the training loop

    # batch_data contains the processed graphs with labels

train_labels

protein_graph_list

from ProteinMPNN.protein_mpnn_utils import ProteinMPNN

import sys
print(sys.path)

# Assuming you've already defined your model above
import torch.optim as optim

# Define the optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Example: Adam optimizer with learning rate of 0.001

# Define the loss function
criterion = torch.nn.CrossEntropyLoss()  # Example for a classification problem

# Training loop
num_epochs = 10  # Set the number of epochs
log_interval = 10  # How often to log progress

for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    for batch_idx, batch_data in enumerate(protein_data_loader):
        batch_data.to(device)  # Move batch data to the appropriate device
        optimizer.zero_grad()  # Zero the gradients before running the backward pass.

        # Forward pass: compute the output of the model
        output = model(batch_data.x, batch_data.edge_index)  # Adjust the forward pass based on your model's method signature

        # Compute the loss
        loss = criterion(output, batch_data.y)  # Ensure you have labels defined in your batch_data as .y
        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters
        optimizer.step()  # Perform a single optimization step (parameter update)

        if batch_idx % log_interval == 0:
            print(f"Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item()}")

print('Training complete.')

# Assuming protein_data_loader is your DataLoader object
for batch_idx, batch_data in enumerate(protein_data_loader):
    print(f"Batch Index: {batch_idx}")

    # Print the type of the batch_data
    print(f"Type of batch_data: {type(batch_data)}")

    # If the batch is a Data object from PyTorch Geometric, it will have various attributes
    # You can print these attributes to understand what each batch contains
    if hasattr(batch_data, 'batch'):
        print("Contains 'batch' tensor which maps data to specific examples in the batch")

    if hasattr(batch_data, 'x'):
        print(f"Feature matrix x: {batch_data.x.size()}")

    if hasattr(batch_data, 'edge_index'):
        print(f"Edge indices: {batch_data.edge_index.size()}")

    if hasattr(batch_data, 'y'):
        print(f"Labels y: {batch_data.y.size()}")

    # Print any other attributes you're interested in
    # Example: Number of nodes and edges if this information is relevant
    if hasattr(batch_data, 'num_nodes'):
        print(f"Number of nodes: {batch_data.num_nodes}")
    if hasattr(batch_data, 'num_edges'):
        print(f"Number of edges: {batch_data.num_edges}")

    # Break after the first batch to only inspect the first one
    break

for batch_idx, data in enumerate(protein_data_loader):
    print(f"Batch {batch_idx}")
    # Print out the attributes you're interested in. For example:
    print("Number of nodes:", data.num_nodes)
    print("Number of edges:", data.num_edges)

    # If you know your data includes specific attributes like 'x' for node features, 'edge_index', or 'y' for labels:
    print("Node features shape:", data.x.shape)
    print("Edge index shape:", data.edge_index.shape)

    # If you have custom attributes like 'cath_id':
    if hasattr(data, 'cath_id'):
        print("cath_id:", data.cath_id)
    else:
        print("cath_id attribute is missing in this batch.")

    # Break after the first batch or more, depending on how many you want to inspect
    if batch_idx == 0:  # Adjust this number to inspect more batches
        break

for i, data in enumerate(protein_graph_list):
    if not hasattr(data, 'cath_id'):
        print(f"Data object at index {i} is missing 'cath_id'")

"""#Tarining the model and perfroming evaluation"""

from torch_geometric.data import Data, DataLoader
import torch
import os
import protein_mpnn_utils

# Initialization of the ProteinMPNN model
model = protein_mpnn_utils.ProteinMPNN(
    num_letters=21,
    node_features=128,
    edge_features=128,
    hidden_dim=256
)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = torch.nn.CrossEntropyLoss()

# # DataLoader setup
# data_loader = DataLoader(protein_graph_list, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Initialize variables for tracking the best validation loss
best_val_loss = float('inf')

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # Switch to training mode
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        output = model(data)  # Adjust this call based on your model's input requirements
        loss = criterion(output, data.y)
        loss.backward()
        optimizer.step()

    # Validation step
    model.eval()  # Switch to evaluation mode
    val_loss = 0
    with torch.no_grad():
        for data in val_loader:
            data = data.to(device)
            output = model(data)
            loss = criterion(output, data.y)
            val_loss += loss.item() * data.num_graphs
    val_loss /= len(val_loader.dataset)
    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss}')

    # Save the model if validation loss has improved
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        model_save_path = '/content/best_model_state_dict.pth'
        torch.save(model.state_dict(), model_save_path)
        print(f'Model saved to {model_save_path} at epoch {epoch+1} with validation loss {val_loss}')