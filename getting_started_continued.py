# -*- coding: utf-8 -*-
"""Getting Started_continued.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189aNGfmSc35ku-bxlmpf53Or88Gt3NhG

#To fasten the processing task
"""

import multiprocessing as mp

try:
    mp.set_start_method('spawn', force=True)
except RuntimeError as e:
    print(f"Error setting multiprocessing start method: {e}")

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/dauparas/ProteinMPNN.git
# %cd /content/

"""#Create the DataSet

"""

# # df = pd.read_csv('/content/cath_w_seqs_share.csv')

# # # Define the splitter with GroupShuffleSplit to ensure no data leakage from homologous superfamily
# # splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)

# # # Generating the splits based on the 'superfamily' column
# # for train_idxs, test_idxs in splitter.split(df, groups=df['superfamily']):
# #     train_df = df.iloc[train_idxs]
# #     val_df, test_df = train_test_split(df.iloc[test_idxs], test_size=0.5, random_state=42)

# # # Extracting IDs and labels
# # train_ids = train_df['cath_id'].tolist()
# # val_ids = val_df['cath_id'].tolist()
# # test_ids = test_df['cath_id'].tolist()

# # # Labels containing 'cath_id' and 'architecture'
# # train_labels = train_df[['cath_id', 'architecture']]
# # val_labels = val_df[['cath_id', 'architecture']]
# # test_labels = test_df[['cath_id', 'architecture']]


# import pandas as pd
# import pandas as pd
# from sklearn.model_selection import GroupShuffleSplit, train_test_split

# # Function to split the dataset
# def load_and_split_data(csv_path):
#     df = pd.read_csv(csv_path)

#     # Define the splitter with GroupShuffleSplit to ensure no data leakage from homologous superfamily
#     splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)

#     # Generating the splits based on the 'superfamily' column
#     for train_idxs, test_idxs in splitter.split(df, groups=df['superfamily']):
#         train_df = df.iloc[train_idxs]
#         test_df = df.iloc[test_idxs]

#     # Further split test set into validation and test
#     val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)

#     # Extracting IDs and labels for datasets
#     train_ids = train_df['cath_id'].tolist()
#     val_ids = val_df['cath_id'].tolist()
#     test_ids = test_df['cath_id'].tolist()

#     # Labels containing 'cath_id' and 'architecture'
#     train_labels = train_df[['cath_id', 'architecture']].values
#     val_labels = val_df[['cath_id', 'architecture']].values
#     test_labels = test_df[['cath_id', 'architecture']].values

#     return train_ids, val_ids, test_ids, train_labels, val_labels, test_labels

# # Use the function to split your dataset
# train_ids, val_ids, test_ids, train_labels, val_labels, test_labels = load_and_split_data('/content/cath_w_seqs_share.csv')

"""#Data Parsing and Preprocessing (centralizing)"""

# # from ProteinMPNN.protein_mpnn_utils  import parse_PDB

# # # Function to parse PDB files and centralize them
# # # def parse_and_centralize_pdb_files(pdb_files):
# # #     pdb_data = []
# # #     for pdb_id in pdb_files:
# # #         pdb_path = f'path/to/pdb/files/{pdb_id}'
# # #         if pdb_info and not isinstance(pdb_info, str):
# # #             for chain_data in pdb_info:
# # #                 # Centralize each chain's coordinates
# # #                 for key, coords_list in chain_data['coords_chain'].items():
# # #                     coords_array = np.array(coords_list)
# # #                     coords_centered = centralize_coords(coords_array)
# # #                     chain_data['coords_chain'][key] = coords_centered.tolist()
# # #             pdb_data.extend(pdb_info)
# # #     return pdb_data

# # def centralize_pdb_coords(pdb_data):
# #     """
# #     Centralizes PDB coordinates for each chain in pdb_data.
# #     pdb_data: List of dictionaries, each containing parsed PDB data for a chain.
# #     """
# #     for chain_data in pdb_data:
# #         for key, coords in chain_data.items():
# #             if isinstance(coords, list):  # Assuming all coordinate data are list types
# #                 coords_array = np.array(coords)
# #                 center = coords_array.mean(axis=0)
# #                 centralized_coords = coords_array - center
# #                 chain_data[key] = centralized_coords.tolist()
# #     return pdb_data

# # import pandas as pd
# # from sklearn.model_selection import GroupShuffleSplit, train_test_split
# # import numpy as np
# # import os

# # # Assuming parse_PDB function is defined elsewhere and correctly imports

# def centralize_coords(coords_array):
#     """
#     Centralizes the coordinates by subtracting the mean.

#     Args:
#     coords_array: numpy array of shape (n_atoms, 3)

#     Returns:
#     numpy array of centralized coordinates
#     """
#     mean = np.mean(coords_array, axis=0)
#     return coords_array - mean

# def parse_and_centralize_pdb(pdb_id, pdb_dir):
#     """
#     Parses and centralizes PDB file based on pdb_id.

#     Args:
#     pdb_id: String, the identifier for the PDB file.
#     pdb_dir: String, directory where PDB files are stored.

#     Returns:
#     Centralized PDB information as a dictionary.
#     """
#     pdb_path = os.path.join(pdb_dir, f"{pdb_id}")
#     pdb_info = parse_PDB(pdb_path)  # Ensure this function is defined to parse PDB files
#     centralized_pdb_info = centralize_pdb_coords(pdb_info)  # Centralize the PDB coordinates
#     return centralized_pdb_info



# # Set your CSV path and PDB directory path
# csv_path = '/content/cath_w_seqs_share.csv'  # Update this path accordingly
# pdb_dir = '/content/drive/MyDrive/PDB_files/pdb_share'  # Update this path to where your PDB files are stored

# import concurrent.futures
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import GroupShuffleSplit, train_test_split
# import os

# import numpy as np

# def parse_PDB_optimized(path_to_pdb, input_chain_list=None, ca_only=False):
#     pdb_dict = {
#         'name': path_to_pdb.split('/')[-1].replace('.pdb', ''),
#         'num_of_chains': 0,
#         'seq': '',
#         'chains': {}
#     }

#     with open(path_to_pdb, 'r') as file:
#         for line in file:
#             if line.startswith(('ATOM', 'HETATM')):
#                 chain_id = line[21]
#                 atom_type = line[12:16].strip()
#                 if ca_only and atom_type != 'CA':
#                     continue

#                 resi = line[17:20].strip()
#                 x, y, z = map(float, [line[30:38], line[38:46], line[46:54]])

#                 if chain_id not in pdb_dict['chains']:
#                     pdb_dict['chains'][chain_id] = {'seq': '', 'coords': []}
#                     pdb_dict['num_of_chains'] += 1

#                 pdb_dict['chains'][chain_id]['coords'].append((x, y, z))
#                 if atom_type == 'CA':  # Assuming seq is derived from CA atoms
#                     pdb_dict['chains'][chain_id]['seq'] += resi  # Conversion of resi to one-letter code needed

#     # Post-processing to convert lists to numpy arrays and centralize
#     for chain_id, chain_data in pdb_dict['chains'].items():
#         coords = np.array(chain_data['coords'])
#         center = coords.mean(axis=0)
#         pdb_dict['chains'][chain_id]['coords'] = coords - center
#         pdb_dict['seq'] += chain_data['seq']  # Concatenate sequences

#     return pdb_dict



# def centralize_pdb_coords(pdb_data):
#
#     # Check if pdb_data is a dictionary before proceeding
#     if isinstance(pdb_data, dict):
#         for chain_id, chain_data in pdb_data['chains'].items():
#             coords = np.array(chain_data['coords'])
#             center = coords.mean(axis=0)
#             centralized_coords = coords - center
#             pdb_data['chains'][chain_id]['coords'] = centralized_coords.tolist()
#         return pdb_data
#     else:
#         # If pdb_data is not a dictionary, return it unchanged.
#         return pdb_data

# def parse_and_centralize_pdb_parallel(pdb_id, pdb_dir):
#     """
#     Parses a PDB file and centralizes its coordinates.
#     If parsing fails, returns an error message or alternative response.
#     """
#     pdb_path = os.path.join(pdb_dir, f"{pdb_id}")
#     pdb_info = parse_PDB_optimized(pdb_path)  # Use the optimized parsing function
#     # Centralize coordinates if pdb_info is correctly parsed; otherwise, return as is.
#     centralized_pdb_info = centralize_pdb_coords(pdb_info)
#     return centralized_pdb_info


# def load_and_process_data_parallel(csv_path, pdb_dir, n_workers=4):
#     df = pd.read_csv(csv_path)
#     splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
#     for train_idxs, test_idxs in splitter.split(df, groups=df['superfamily']):
#         train_df = df.iloc[train_idxs]
#         test_df = df.iloc[test_idxs]
#     val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)

#     datasets = {'train': train_df, 'val': val_df, 'test': test_df}
#     processed_datasets = {}

#     with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as executor:
#         for name, dataset_df in datasets.items():
#             futures = [executor.submit(parse_and_centralize_pdb_parallel, pdb_id, pdb_dir) for pdb_id in dataset_df['cath_id'].tolist()]
#             results = [f.result() for f in concurrent.futures.as_completed(futures)]
#             labels = dataset_df[['cath_id', 'architecture']].values
#             processed_datasets[name] = {'data': results, 'labels': labels}

#     return processed_datasets

# def load_and_split_data(csv_path, pdb_dir):
#     df = pd.read_csv(csv_path)
#     splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)

#     for train_idxs, test_idxs in splitter.split(df, groups=df['superfamily']):
#         train_df = df.iloc[train_idxs]
#         test_df = df.iloc[test_idxs]

#     val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)

#     datasets = {}
#     for name, dataset_df in zip(['train', 'val', 'test'], [train_df, val_df, test_df]):
#         pdb_data = [parse_and_centralize_pdb(pdb_id, pdb_dir) for pdb_id in dataset_df['cath_id'].tolist()]
#         labels = dataset_df[['cath_id', 'architecture']].values
#         datasets[name] = {'data': pdb_data, 'labels': labels}

#     return datasets

# # Usage:
# datasets = load_and_process_data_parallel(csv_path, pdb_dir)

# # # Load and preprocess datasets
# datasets = load_and_split_data(csv_path, pdb_dir)

# # # Now, datasets contain 'train', 'val', and 'test' keys with their respective data and labels

# import numpy as np
# import torch
# from ProteinMPNN.protein_mpnn_utils import ProteinFeatures, CA_ProteinFeatures


# import numpy as np
# import torch

# def centralize_coords(coords):
#     """Centralize coordinates by subtracting the mean."""
#     mean_coords = np.mean(coords, axis=0)
#     return coords - mean_coords

# def one_hot_encode_sequence(sequence, alphabet="ACDEFGHIKLMNPQRSTVWY"):
#     """One-hot encode a protein sequence."""
#     encoding = np.zeros((len(sequence), len(alphabet)))
#     for i, amino_acid in enumerate(sequence):
#         if amino_acid in alphabet:
#             encoding[i, alphabet.index(amino_acid)] = 1
#     return encoding

# def compute_distance_matrix(coords):
#     """Compute the distance matrix for the given coordinates."""
#     num_atoms = coords.shape[0]
#     dist_matrix = np.zeros((num_atoms, num_atoms))
#     for i in range(num_atoms):
#         for j in range(i + 1, num_atoms):  # optimization to compute only half since the matrix is symmetric
#             dist = np.linalg.norm(coords[i] - coords[j])
#             dist_matrix[i, j] = dist_matrix[j, i] = dist
#     return dist_matrix

# #trying this
# def validate_and_extract_sequence(cath_id, sequence_dict):
#     """
#     Validates and extracts sequence for the given cath_id from the sequence_dict.
#     Returns a valid sequence or raises an error if the sequence is missing or invalid.
#     """
#     sequence = sequence_dict.get(cath_id)
#     if sequence is None:
#         raise ValueError(f"Sequence for CATH ID {cath_id} is missing.")
#     if not isinstance(sequence, str) or len(sequence) == 0:
#         raise ValueError(f"Invalid sequence for CATH ID {cath_id}: {sequence}")
#     return sequence

# def extract_features_for_mpnn(pdb_data, sequence_dict, ca_only=True):
#     """Extract node and edge features from centralized PDB data using sequence dictionary."""
#     node_features_list = []
#     edge_features_list = []
#     adjacency_list = []

#
#     for chain_data in pdb_data:
#         print(chain_data)
#
#         cath_id = chain_data['name']  # Adjust 'name' as per your actual data structure

#         # Fetch sequence using cath_id, handle missing sequences gracefully
#         sequence = sequence_dict.get(cath_id, None)
#         if sequence is None or 'Nan':
#             print(f"Sequence not found for cath_id: {cath_id}")
#             continue  # Skip this entry if sequence not found

#
#         coords = np.array(chain_data['coords'])

#         # Generate node features from the sequence
#         node_features = one_hot_encode_sequence(sequence)
#         node_features_list.append(torch.tensor(node_features, dtype=torch.float))

#         # Compute edge features and adjacency based on the coordinates
#         dist_matrix = compute_distance_matrix(coords)
#         edge_features = 1 / (dist_matrix + 1e-6)  # Simple inverse distance as edge features
#         adjacency = (dist_matrix < 8.0).astype(np.float64)  # Simple cutoff for adjacency, ensuring dtype compatibility
#         edge_features_list.append(torch.tensor(edge_features, dtype=torch.float))
#         adjacency_list.append(torch.tensor(adjacency, dtype=torch.float))

#     return node_features_list, edge_features_list, adjacency_list


# # Load the sequences from CSV
# sequence_df = pd.read_csv('/content/cath_w_seqs_share.csv')

# # Ensure no leading/trailing whitespaces in 'cath_id' and 'sequences'
# sequence_df['cath_id'] = sequence_df['cath_id'].str.strip()
# sequence_df['sequences'] = sequence_df['sequences'].str.strip()

# sequence_dict = pd.Series(sequence_df.sequences.values, index=sequence_df.cath_id).to_dict()

# # Now, sequence_dict can be used to access sequences by cath_id
# datasets = load_and_split_data(csv_path, pdb_dir)


# train_features = extract_features_for_mpnn(datasets['train']['data'], sequence_dict=sequence_dict,ca_only=True)
# # val_features = extract_features_for_mpnn(datasets['val']['data'], sequence_dict, ca_only=True)
# # test_features = extract_features_for_mpnn(datasets['test']['data'], sequence_dict, ca_only=True)
# # `train_features` now ready for input into ProteinMPNN model

sequence_dict

"""#Different versions of Datalaoding, parsing and preprocessing method to endure data fits the model input"""

df = pd.read_csv('/content/cath_w_seqs_share.csv')
cath_id_to_find = '2gnxA01'

# Check if the cath_id exists in the DataFrame
if cath_id_to_find in df['cath_id'].values:
    # Extract and print the sequence for the given cath_id
    sequence = df.loc[df['cath_id'] == cath_id_to_find, 'sequences'].iloc[0]
    print(f"Sequence for cath_id {cath_id_to_find}: {sequence}")
else:
    print(f"cath_id {cath_id_to_find} not found in the CSV file.")

import concurrent.futures
import os
import pandas as pd
import numpy as np
import torch
from concurrent.futures import ProcessPoolExecutor
from sklearn.model_selection import GroupShuffleSplit, train_test_split


def parse_PDB_optimized(path_to_pdb):
    pdb_dict = {
        'name': path_to_pdb.split('/')[-1].replace('.pdb', ''),
        'num_of_chains': 0,
        'seq': '',
        'chains': {}
    }

    with open(path_to_pdb, 'r') as file:
        for line in file:
            if line.startswith(('ATOM', 'HETATM')):
                chain_id = line[21]
                atom_type = line[12:16].strip()
                if ca_only and atom_type != 'CA':
                    continue

                resi = line[17:20].strip()
                x, y, z = map(float, [line[30:38], line[38:46], line[46:54]])

                if chain_id not in pdb_dict['chains']:
                    pdb_dict['chains'][chain_id] = {'seq': '', 'coords': []}
                    pdb_dict['num_of_chains'] += 1

                pdb_dict['chains'][chain_id]['coords'].append((x, y, z))
                if atom_type == 'CA':  # Assuming seq is derived from CA atoms
                    pdb_dict['chains'][chain_id]['seq'] += resi  # Conversion of resi to one-letter code needed

    # Post-processing to convert lists to numpy arrays and centralize
    for chain_id, chain_data in pdb_dict['chains'].items():
        coords = np.array(chain_data['coords'])
        center = coords.mean(axis=0)
        pdb_dict['chains'][chain_id]['coords'] = coords - center
        pdb_dict['seq'] += chain_data['seq']  # Concatenate sequences

    return pdb_dict

def centralize_pdb_coords(pdb_data):
    """Centralizes PDB coordinates for each chain in pdb_data if pdb_data is a dictionary."""
    if isinstance(pdb_data, dict):
        for chain_id, chain_data in pdb_data['chains'].items():
            coords = np.array(chain_data['coords'])
            center = coords.mean(axis=0)
            centralized_coords = coords - center
            pdb_data['chains'][chain_id]['coords'] = centralized_coords.tolist()
    return pdb_data

def load_sequence_dict(csv_path):
    sequence_df = pd.read_csv(csv_path).dropna(subset=['sequences'])
    sequence_dict = pd.Series(sequence_df.sequences.values, index=sequence_df.cath_id).to_dict()
    return sequence_dict

def one_hot_encode_sequence(sequence, alphabet="ACDEFGHIKLMNPQRSTVWY"):
    encoding = np.zeros((len(sequence), len(alphabet)))
    for i, amino_acid in enumerate(sequence):
        if amino_acid in alphabet:
            encoding[i, alphabet.index(amino_acid)] = 1
    return encoding

def compute_distance_matrix(coords):
    num_atoms = coords.shape[0]
    dist_matrix = np.zeros((num_atoms, num_atoms))
    for i in range(num_atoms):
        for j in range(i + 1, num_atoms):
            dist = np.linalg.norm(coords[i] - coords[j])
            dist_matrix[i, j] = dist_matrix[j, i] = dist
    return dist_matrix

def process_pdb_file(pdb_id, pdb_dir, sequence_dict):
    pdb_path = os.path.join(pdb_dir, f"{pdb_id}")
    if not os.path.exists(pdb_path):
        print(f"File not found: {pdb_path}")
        return None
    pdb_data = parse_PDB_optimized(pdb_path)
    sequence = sequence_dict.get(pdb_id, "")
    if sequence:
        pdb_data['sequence'] = sequence
        centralized_pdb_data = centralize_pdb_coords(pdb_data)
        return centralized_pdb_data
    else:
        print(f"Sequence not found for cath_id: {pdb_id}")
        return None

def load_and_process_data_parallel(pdb_ids, pdb_dir, sequence_dict, n_workers=4):
    processed_data = []
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        futures = [executor.submit(process_pdb_file, pdb_id, pdb_dir, sequence_dict) for pdb_id in pdb_ids]
        for future in futures:
            result = future.result()
            if result:
                processed_data.append(result)
    return processed_data

def split_data(csv_path):
    df = pd.read_csv(csv_path)
    splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
    for train_idxs, test_idxs in splitter.split(df, groups=df['superfamily']):
        train_df = df.iloc[train_idxs]
        test_df = df.iloc[test_idxs]
    val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)
    return {'train': train_df.cath_id.tolist(), 'val': val_df.cath_id.tolist(), 'test': test_df.cath_id.tolist()}


def extract_features_for_mpnn(pdb_data, sequence_dict):
    """Extracts features suitable for MPNN input from PDB data."""
    node_features_list = []
    edge_features_list = []
    adjacency_list = []

    for pdb_entry in pdb_data:
        sequence = sequence_dict.get(pdb_entry['name'], "")
        if not sequence:
            continue  # Skip if no sequence found

        for chain_id, chain_data in pdb_entry['chains'].items():
            coords = np.array(chain_data['coords'])
            node_features = one_hot_encode_sequence(sequence)
            node_features_list.append(torch.tensor(node_features, dtype=torch.float))

            dist_matrix = compute_distance_matrix(coords)
            edge_features = 1 / (dist_matrix + 1e-6)
            adjacency = (dist_matrix < 8.0).astype(np.float64)

            edge_features_list.append(torch.tensor(edge_features, dtype=torch.float))
            adjacency_list.append(torch.tensor(adjacency, dtype=torch.float))

    return node_features_list, edge_features_list, adjacency_list


csv_path = '/content/cath_w_seqs_share.csv'
pdb_dir = '/content/drive/MyDrive/PDB_files/pdb_share'

# Loading the sequence dictionary
# sequence_dict = load_sequence_dict(csv_path)

# # Splitting the data
# data_splits = split_data(csv_path)

# # Processing PDB data in parallel
# processed_train_data = load_and_process_data_parallel(data_splits['train'], pdb_dir, sequence_dict, n_workers=4)
# processed_val_data = load_and_process_data_parallel(data_splits['val'], pdb_dir, sequence_dict, n_workers=4)
# processed_test_data = load_and_process_data_parallel(data_splits['test'], pdb_dir, sequence_dict, n_workers=4)

# # Extracting features for MPNN
# train_features = extract_features_for_mpnn(processed_train_data, sequence_dict)
# val_features = extract_features_for_mpnn(processed_val_data, sequence_dict)
# test_features = extract_features_for_mpnn(processed_test_data, sequence_dict)

# import os
# import pandas as pd
# import numpy as np
# import torch
# from concurrent.futures import ProcessPoolExecutor
# from sklearn.model_selection import GroupShuffleSplit, train_test_split


# def load_sequence_dict(csv_path):
#     sequence_df = pd.read_csv(csv_path).dropna(subset=['sequences'])
#     sequence_dict = sequence_df.set_index('cath_id')['sequences'].to_dict()
#     return sequence_dict
# # Load sequence dictionary
# sequence_dict = load_sequence_dict(csv_path)

# def process_pdb_file(pdb_file, pdb_dir, sequence_dict):
#     pdb_path = os.path.join(pdb_dir, f"{pdb_file}.pdb")
#     if not os.path.exists(pdb_path):
#         return None
#     pdb_data = parse_PDB_optimized(pdb_path)
#     sequence = sequence_dict.get(pdb_data['name'], "")
#     if sequence:
#         pdb_data['sequence'] = sequence
#         centralized_pdb_data = centralize_pdb_coords(pdb_data)
#         return centralized_pdb_data
#     return None

# def load_and_process_data_parallel(pdb_ids, pdb_dir, sequence_dict, n_workers=4):
#     with ProcessPoolExecutor(max_workers=n_workers) as executor:
#         futures = {executor.submit(process_pdb_file, pdb_id, pdb_dir, sequence_dict): pdb_id for pdb_id in pdb_ids}
#         return [future.result() for future in futures if future.result()]

# def split_data(csv_path):
#     df = pd.read_csv(csv_path)
#     splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
#     for train_idxs, test_idxs in splitter.split(df, groups=df['superfamily']):
#         train_df = df.iloc[train_idxs]
#         test_df = df.iloc[test_idxs]
#     val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)
#     return train_df['cath_id'].tolist(), val_df['cath_id'].tolist(), test_df['cath_id'].tolist()

# csv_path = '/content/cath_w_seqs_share.csv'
# pdb_dir = '/content/drive/MyDrive/PDB_files/pdb_share'

# # Load sequence dictionary
# sequence_dict = load_sequence_dict(csv_path)

# # Splitting the data
# train_ids, val_ids, test_ids = split_data(csv_path)

# # Processing PDB data in parallel
# processed_train_data = load_and_process_data_parallel(train_ids, pdb_dir, sequence_dict, n_workers=4)
# processed_val_data = load_and_process_data_parallel(val_ids, pdb_dir, sequence_dict, n_workers=4)
# processed_test_data = load_and_process_data_parallel(test_ids, pdb_dir, sequence_dict, n_workers=4)

# # Extracting features for MPNN
# train_features = extract_features_for_mpnn(processed_train_data, sequence_dict)
# val_features = extract_features_for_mpnn(processed_val_data, sequence_dict)
# test_features = extract_features_for_mpnn(processed_test_data, sequence_dict)

sequence_dict
processed_train_data

!pip install biopython

from Bio.PDB import *

def parse_PDB_with_bioPython(path_to_pdb):
    parser = PDBParser()
    structure = parser.get_structure('PDB_structure', path_to_pdb)
    model = structure[0]

    pdb_info = {
        'name': os.path.basename(path_to_pdb).replace('.pdb', ''),
        'num_of_chains': len(model),
        'seq': '',
        'chains': {}
    }

    for chain in model:
        chain_id = chain.id
        seq = ''
        coords = []
        for residue in chain:
            if is_aa(residue):
                seq += Polypeptide.three_to_one(residue.resname)
                if 'CA' in residue:
                    ca_atom = residue['CA']
                    coords.append((ca_atom.coord[0], ca_atom.coord[1], ca_atom.coord[2]))

        pdb_info['chains'][chain_id] = {
            'seq': seq,
            'coords': np.array(coords) - np.mean(coords, axis=0)  # Centralize coordinates
        }
        pdb_info['seq'] += seq  # Concatenate sequences from all chains

    return pdb_info

def process_pdb_data_1(pdb_id, pdb_dir, sequence_dict):
    pdb_path = os.path.join(pdb_dir, f"{pdb_id}.pdb")
    pdb_data = parse_pdb_with_biopython(pdb_path)

    processed_chains = []
    for chain_id, chain_data in pdb_data['chains'].items():
        sequence = chain_data['seq']

        node_features = one_hot_encode_sequence(sequence)
        centralized_coords = centralize_coords(chain_data['coords'])
        dist_matrix = compute_distance_matrix(centralized_coords)
        edge_features = 1 / (dist_matrix + 1e-6)
        adjacency = (dist_matrix < 8.0).astype(np.float32)
        processed_chains.append((torch.tensor(node_features, dtype=torch.float32),
                                 torch.tensor(edge_features, dtype=torch.float32),
                                 torch.tensor(adjacency, dtype=torch.float32)))
    return processed_chains

import os
import numpy as np
import concurrent.futures
import pandas as pd
import torch
from Bio.PDB import *
from sklearn.model_selection import GroupShuffleSplit, train_test_split
from concurrent.futures import ProcessPoolExecutor
from Bio.SeqUtils import seq3
import warnings
from Bio.PDB.PDBExceptions import PDBConstructionWarning
warnings.filterwarnings('ignore', category= PDBConstructionWarning)


pdb_dir = '/content/drive/MyDrive/PDB_files/pdb_share'
csv_path = '/content/cath_w_seqs_share.csv'

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Parsing PDB files using BioPython
def parse_PDB_with_bioPython(path_to_pdb):
    parser = PDBParser()
    structure = parser.get_structure('PDB_structure', path_to_pdb)
    model = structure[0]  # Assuming only one model in the PDB file

    pdb_info = {'name': os.path.basename(path_to_pdb), 'chains': {}}
    for chain in model:
        if not chain.id.strip():
            continue  # Skip if chain ID is empty or not defined properly
        seq = ''
        coords = []
        for residue in chain:
            if is_aa(residue, standard=True):
                try:
                    # Convert three-letter code to one-letter code
                    one_letter_code = seq3(residue.get_resname()).upper()
                    seq += one_letter_code
                    if 'CA' in residue:
                        ca_atom = residue['CA']
                        coords.append(ca_atom.coord)
                except KeyError:
                    continue  # Skip unknown residues

        pdb_info['chains'][chain.id] = {'seq': seq, 'coords': np.array(coords)}

    return pdb_info


# Centralize coordinates
def centralize_coords(coords):
    mean_coords = np.mean(coords, axis=0)
    return coords - mean_coords

# One-hot encode sequence
def one_hot_encode_sequence(sequence, alphabet="ACDEFGHIKLMNPQRSTVWY"):
    encoding = np.zeros((len(sequence), len(alphabet)))
    for i, amino_acid in enumerate(sequence):
        if amino_acid in alphabet:
            encoding[i, alphabet.index(amino_acid)] = 1
    return encoding

# Compute distance matrix
def compute_distance_matrix(coords):
    num_atoms = coords.shape[0]
    dist_matrix = np.zeros((num_atoms, num_atoms))
    for i in range(num_atoms):
        for j in range(i + 1, num_atoms):
            dist = np.linalg.norm(coords[i] - coords[j])
            dist_matrix[i, j] = dist_matrix[j, i] = dist
    return dist_matrix

# Load sequence dictionary from CSV
def load_sequence_dict(csv_path):
    sequence_df = pd.read_csv(csv_path).dropna(subset=['sequences'])
    sequence_dict = sequence_df.set_index('cath_id')['sequences'].to_dict()
    return sequence_dict

amino_acid_vocab = {
    'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4,
    'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9,
    'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14,
    'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19
}

def encode_sequence(sequence, vocab):
    return [vocab[aa] for aa in sequence if aa in vocab]

# Process PDB data (parallel execution)
def process_pdb_data_2(pdb_id, pdb_dir, sequence_dict):
    pdb_path = os.path.join(pdb_dir, f"{pdb_id}")
    if not os.path.exists(pdb_path):
        print(f"File not found: {pdb_path}")
        return None
    pdb_data = parse_PDB_with_bioPython(pdb_path)
    processed_data = []
    for chain_id, chain_data in pdb_data['chains'].items():
        centralized_coords = centralize_coords(chain_data['coords'])
        sequence = sequence_dict.get(pdb_id, '')
        encoded_sequence = encode_sequence(sequence, amino_acid_vocab)  # Encode it
        encoded_sequence_tensor = torch.tensor(encoded_sequence, dtype=torch.long)
        chain_seq = chain_data['seq']
        # print(chain_seq)
        node_features = one_hot_encode_sequence(chain_seq)
        dist_matrix = compute_distance_matrix(centralized_coords)
        edge_features = 1 / (dist_matrix + 1e-6)
        # adjacency = (dist_matrix < 8.0).astype(np.float32)
        processed_data.append((torch.tensor(node_features, dtype=torch.float32),
                               encoded_sequence_tensor,
                               torch.tensor(edge_features, dtype=torch.float32)))



    return processed_data

# Load and process data in parallel
def load_and_process_data_in_parallel(ids_list, labels, pdb_dir, sequence_dict, n_workers=2):
    # Process PDB files and fetch corresponding labels
    processed_data = []
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        futures = [executor.submit(process_pdb_data_2, pdb_id, pdb_dir, sequence_dict) for pdb_id in ids_list]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result:
                processed_data.append(result)

    # print(processed_data)
    return processed_data, labels

# Load sequence dictionary
sequence_dict = load_sequence_dict(csv_path)
print(sequence_dict)


# Split dataset considering homologous superfamilies
def split_data_homologous_with_labels(csv_path):
    df = pd.read_csv(csv_path)

    df['architecture'] = df['architecture'].astype('category').cat.codes

    splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
    train_idxs, test_idxs = next(splitter.split(df, groups=df['superfamily']))
    train_df = df.iloc[train_idxs]
    test_df = df.iloc[test_idxs]

    val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)

    splits = {
        'train': (train_df['cath_id'].tolist(), train_df['architecture'].tolist()),
        'val': (val_df['cath_id'].tolist(), val_df['architecture'].tolist()),
        'test': (test_df['cath_id'].tolist(), test_df['architecture'].tolist())
    }
    return splits


# Load and process data considering homologous superfamilies
# def load_and_process_homologous_data(csv_path, pdb_dir):
#     ids_splits = split_data_homologous_with_labels(csv_path)
#     sequence_dict = load_sequence_dict(csv_path)



from torch.utils.data.dataloader import default_collate

!pip install nbimporter

# import nbimporter
# import sys

# sys.path.append('/content/drive/MyDrive/Colab Notebooks')

# import helper
# !jupyter nbconvert --to script '/content/drive/MyDrive/Colab Notebooks/helper.ipynb' --output '/content/drive/MyDrive/Colab Notebooks/helper.py'

# import sys


# sys.path.append('/content/drive/MyDrive/Colab Notebooks')

# from helper import CustomProteinDataset

import sys
sys.path.append('/content/drive/MyDrive/Colab Notebooks')
from helper import CustomProteinDataset

from torch.utils.data import DataLoader
from ProteinMPNN.protein_mpnn_utils import ProteinFeatures, CA_ProteinFeatures, ProteinMPNN, StructureDatasetPDB, StructureLoader
from pathlib import Path

# import torch.multiprocessing as mp

class CustomProteinDataset(torch.utils.data.Dataset):
    def __init__(self, pdb_list,  labels):
        # # Validate each item in pdb_list
        # for i, item in enumerate(pdb_list):
        #     if not (isinstance(item, tuple) and len(item) == 3):
        #         raise ValueError(f"Item at index {i} does not match expected format: {item}")
        self.data = pdb_list
        self.labels = labels
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Extract the first item from the list which is the tuple of tensors
        item = self.data[idx][0]

        # If this is not the case, further adjustments may be needed.
        node_features, sequence, edge_features = item
        label = self.labels[idx]
        label = torch.tensor(label, dtype=torch.long)
        X = node_features
        S = sequence
        mask = torch.ones(X.size(0), dtype=torch.float32)  # Assuming all nodes are valid
        mask = mask.unsqueeze(0)
        chain_M = torch.ones(X.size(0), dtype=torch.float32)  # Assuming a single chain
        residue_idx = torch.arange(X.size(0), dtype=torch.long)
        chain_encoding_all = torch.zeros(X.size(0), dtype=torch.float32)  # Assuming a single chain
        randn = torch.randn(X.size(0), dtype=torch.float32)  # Random noise

        return X, S, mask, chain_M, residue_idx, chain_encoding_all, randn, edge_features, label

def custom_collate_fn(batch):
    # Separate the node features, edge features, and labels.
    node_features = [item[0] for item in batch]
    sequence = [item[1] for item in batch]
    mask = [item[2] for item in batch]
    chain_M = [item[3] for item in batch]
    residue_idx = [item[4] for item in batch]
    chain_encoding_all = [item[5] for item in batch]
    randn = [item[6] for item in batch]
    edge_features = [item[7] for item in batch]
    labels = [item[8] for item in batch]


    # Only collate the labels as they can be stacked (assuming they are scalar values or fixed-size vectors).
    labels = default_collate(labels)

    return node_features, sequence, mask, chain_M, residue_idx, chain_encoding_all, randn, edge_features, labels

# Convert to StructureDatasetPDB and loaders
splits = split_data_homologous_with_labels(csv_path)
train_ids, train_labels = splits['train']
# val_ids, val_labels = splits['val']
# test_ids, test_labels = splits['test']

sequence_dict = load_sequence_dict(csv_path)
train_data, train_labels = load_and_process_data_in_parallel(train_ids, train_labels, pdb_dir, sequence_dict, n_workers=2)
# val_data, val_labels = load_and_process_data_in_parallel(val_ids, val_labels, pdb_dir, sequence_dict, n_workers=2)
# test_data, test_labels = load_and_process_data_in_parallel(test_ids, test_labels, pdb_dir, sequence_dict, n_workers=2)

# print("Train_data:", train_data, train_labels)
train_dataset = CustomProteinDataset(train_data, train_labels)
# val_dataset = CustomProteinDataset(val_data, val_labels)
# test_dataset = CustomProteinDataset(test_data, test_labels)

import pickle

#save the data
train_data_path = '/content/sample_data/train_data.pkl'
train_labels_path = '/content/sample_datatrain_labels.pkl'

# Save train_data
with open(train_data_path, 'wb') as f:
    pickle.dump(train_data, f)

# Save train_labels
with open(train_labels_path, 'wb') as f:
    pickle.dump(train_labels, f)

num_letters = 20  # Adjust based on your data
node_features = 128
edge_features = 128
hidden_dim = 256
num_encoder_layers = 3
num_decoder_layers = 3
vocab = 21
k_neighbors = 30
augment_eps = 0.05
dropout = 0.1
ca_only = True
batch_size = 32

# Initialize the model
model = ProteinMPNN(num_letters, node_features, edge_features, hidden_dim, num_encoder_layers,
                    num_decoder_layers, vocab, k_neighbors, augment_eps, dropout, ca_only)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)
# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)

def _dist(self, X, mask, eps=1e-6):
    """ Pairwise euclidean distances """
    mask = mask.unsqueeze(-1)  # to make sure mask is [batch_size, num_nodes, 1]
    dX = X.unsqueeze(1) - X.unsqueeze(2)  # Shape: [batch_size, num_nodes, num_nodes, features]
    D = torch.sqrt((dX ** 2).sum(-1) + eps)  # Euclidean distance

    mask_2D = mask * mask.transpose(1, 2)  # Apply mask to both dimensions
    D *= mask_2D  # Apply mask to distances

    return D

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    total_loss = 0

    for batch_idx, (X, S, mask, chain_M, residue_idx, chain_encoding_all, randn, edge_features, label) in enumerate(train_loader):
        labels = label.to(device)  # Move labels to the correct device

        # Assuming your model can process one graph at a time:
        batch_loss = 0
        for node_features, seq_features, m_feat, chain_feat, residue_feat, chain_encod_feat, rand_feat, edge_features in zip(X, S, mask, chain_M, residue_idx, chain_encoding_all, randn, edge_features):
            # Move each graph's data to the device. This step is necessary if you're using a GPU.
            node_features = node_features.to(device)
            seq_features = seq_features.to(device)
            m_feat = m_feat.to(device)
            chain_feat = chain_feat.to(device)
            residue_feat = residue_feat.to(device)
            chain_encod_feat = chain_encod_feat.to(device)
            rand_feat = rand_feat.to(device)
            edge_features = edge_features.to(device)
            print(node_features.shape)
            print(seq_features.shape)
            print(m_feat.shape)
            print(chain_feat.shape)
            print(residue_feat.shape)
            print(chain_encod_feat.shape)
            print(rand_feat.shape)

            inputs = {
            "X": node_features,
            "S": seq_features,
            "mask": m_feat,
            "chain_M": chain_feat,
            "residue_idx": residue_feat,
            "chain_encoding_all": chain_encod_feat,
            "randn": rand_feat,
            "use_input_decoding_order": False,  # Letting the model decide the order
            # "decoding_order: None
        }
            # Forward pass for this graph
            outputs = model(**inputs)
            loss = criterion(outputs, labels[batch_idx].unsqueeze(0))  # Adjust the indexing and unsqueeze as needed

            # Backpropagation and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            batch_loss += loss.item()

        total_loss += batch_loss / len(node_features)  # Average loss per graph in the batch

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')

"""#Checking data formats and structure of the first few as the processed train set is too large to store."""

def sample_data_check(processed_data, sample_size=5):
    for i, item in enumerate(processed_data[:sample_size]):
        print(f"Sample {i}: Type: {type(item)}, Length: {len(item) if isinstance(item, tuple) else 'N/A'}, Content: {item}")

# Apply function after processing data but before passing it to the DataLoader
sample_data_check(train_data)
# sample_data_check(val_data)
# sample_data_check(test_data)


def log_sample_data(processed_data, file_path, sample_size=5):
    with open(file_path, 'w') as f:
        for i, item in enumerate(processed_data[:sample_size]):
            f.write(f"Sample {i}: Type: {type(item)}, Length: {len(item) if isinstance(item, tuple) else 'N/A'}, Content: {item}\n")